**第3章　LLM的内部机制**

# Q20：大模型怎么知道它的输出该结束了？
大模型知道它的输出该结束了，主要通过以下几种机制。

1. **停止标记**：大模型在训练时会学习特殊的停止标记，这些标记告诉模型输出应该结束。当模型生成回答时，当它预测下一个最应该出现的词是停止标记时，生成过程就自动停止了。
2. **最大长度限制**：模型有预设的最大生成长度限制。当模型的输出达到了这个预设的上限时会自动停止生成。
# Q21：训练时如何防止模型看到未来的词元？
在训练语言模型（尤其是像GPT这样的自回归模型）时，为了防止模型在预测当前词元时“偷看”到未来的词元，最关键的技术是使用注意力掩码（Attention Mask）或因果掩码（Causal Mask）。这个过程发生在 Transformer 模型的自注意力（Self-Attention）层中。

**核心思想**：对于序列中的任何一个词元（token），模型在计算它的表征时，只能关注它自己以及它前面的所有词元，绝对不能关注它后面的词元。

1. **因果掩码（Casual Mask）**：通过下三角矩阵确保模型只能看到当前及之前的信息。
2. **自回归特性**：这种设计使模型逐词生成任务。
# Q22：注意力机制是如何计算上下文各个词元之间的相关性的？每个注意力头是只关注一个词元吗？softmax之前为什么要除以 $\sqrt{d_k}$?
1. **相关性计算**：注意力机制通过查询（Query）、键（Key）、V（Value）三元组来计算词元之间的相关性。注意力机制的核心思想是为序列中的每个词元（token）生成一个加权的上下文表示。这个权重代表了序列中其他所有词元（包括它自己）与当前词元的相关性。这个过程是通过三个向量来实现的：查询（Query, Q）、键（Key, K） 和 值（Value, V）。
   * 查询 (Query, Q): 代表当前正在处理的词元。它是“探寻者”，提出问题：“根据我自己的身份，过去哪些信息对于预测下一个词元最重要？”
   * 键 (Key, K): 代表一个过去词元的“标签”或“标识符”。Q 会与它进行比较，来计算注意力分数。它回答了“这个过去的词元与我（Q）相关吗？”这个问题。
   * 值 (Value, V): 代表一个过去词元的实际“内容”或信息。一旦 Q 找到了相关的 K，与之配对的 V 就会被传递过去，作为合成下一个词元的参考。
3. **注意力头**：每个注意力头不是只关注一个词元，每个头关注整个序列中的所有词元，但会学习不同的关注模式（语法、语义、位置等）。
4. **除以 $\sqrt{d_k}$**：数学上为了控制方差，防止梯度消失，确保训练稳定性。这个操作被称为缩放点积注意力（Scaled Dot-Product Attention）。
# Q23：Q和K在注意力的表达式里看起来是对称的，但KV缓存里为什么只有KV，没有Q？
简单来说：Q 代表“现在”，K 和 V 代表“过去”。 在生成文本时，我们只需要缓存“过去”的信息。缓存旧的Q向量是无用的，因为你永远不需要从过去的位置再次提出相同的问题。你永远是从当前位置，对整个过去提问。因此，虽然 Q 和 K 在数学公式上是对称的，但它们在语言生成这个具有时间依赖性的因果过程中，所扮演的角色是完全不对称的。KV缓存正是利用了这种操作上的不对称性，通过避免重复计算过去的状态，极大地提升了生成效率。

| 组件        | 生成过程中的角色           | 为什么被缓存（或不被缓存）                                                                 |
|-------------|----------------------------|--------------------------------------------------------------------------------------------|
| **键 (K)**  | 代表一个**过去**的词元     | **需要缓存**：因为“现在”的词元需要和所有“过去”的词元进行比较，以计算相关性。               |
| **值 (V)**  | 代表一个**过去**词元的内容 | **需要缓存**：因为这是“现在”的词元最终要从“过去”提取的实际信息。                         |
| **查询 (Q)**| 代表**当前**的词元，正在提问 | **无需缓存**：它非常短暂，只在当前步使用。下一步会生成一个全新的 Q，旧的 Q 就没用了。     |

 虽然Q和K在数学表达式上是对称的，但在计算图和内存使用模式上存在本质差异：
  - Q：每个时间步都需要重新计算，是瞬态的
  - K和V：一旦计算就可以被后续所有时间步复用，是持久的
    
 这种设计使得KV缓存能够显著提高推理效率，减少重复计算，同时保持最小的内存开销。
# Q24：如果没有KV缓存，推理性能会降低多少？
没有KV缓存会导致推理性能呈二次方下降，从O(L)变为O(L²)。

# Q25：为什么Transformer中需要残差连接？
**残差连接**：就是将一个模块的输入直接加到这个模块的输出上。用公式表示就是：Output = Layer(x) + x
   * 其中 x 是输入，Layer(x) 是一个或多个网络层（比如自注意力层或前馈网络层）对 x 的处理结果。
     
引入残差连接主要是为了解决深度神经网络中普遍存在的两个核心问题：
   * 解决梯度消失的问题
   * 避免网络退化的问题

在 Transformer 中使用残差连接有以下核心好处：
   * 解决梯度消失，让梯度能够有效地在深层网络中传播。
   * 允许构建非常深的网络，这是 Transformer 模型能够堆叠多个编码器和解码器层的关键。
   * 加速模型训练，因为梯度流更顺畅，收敛更快。
   * 保留原始信息，将输入 x 的信息直接传递到下一层，避免在多层转换中丢失关键特征。

# Q26：Transformer中的LayerNorm跟ResNet中的BatchNorm有什么区别，为什么Llama-3换用了RMSNorm？
简单来说，这三者是归一化技术发展的不同阶段，核心目标都是为了让模型训练更稳定、更快速，但它们的策略和计算效率不同。
   - BatchNorm (BN)：元老级的归一化方法，为CNN设计。
   - LayerNorm (LN)：为解决BN在RNN和Transformer上的水土不服而生。
   - RMSNorm（Root Mean Square Normalization，均方根归一化）：LayerNorm的“减配提速”版，在现代LLM中表现优异，被Llama-3等模型采用。

1. Transformer中的LayerNorm跟ResNet中的BatchNorm有什么区别？从 BatchNorm 到 LayerNorm 是为了适应模型架构（从CNN到Transformer）的必要变革。
2. 为什么Llama-3换用了RMSNorm？从 LayerNorm 到 RMSNorm 则是大模型时代追求极致效率的优化选择。Llama-3采用RMSNorm，是在保证模型性能不下降的前提下，通过简化计算来大幅提升训练和推理速度，这对于动辄需要数千块GPU训练数周的SOTA模型来说，是至关重要的。
# Q27：Transformer中前馈神经网络的作用是什么？注意力层中已经有softmax非线性层，那么前馈神经网络是否必要？
* **Transformer中前馈神经网络（Feed-Forward Network, FFN）的作用**：记忆和插值。模型能够利用相同的机制在数据点之间进行插值，识别更复杂的模式，从而实现泛化。在 Transformer 的每个编码器（Encoder）和解码器（Decoder）层中，FFN 都位于自注意力（Self-Attention）层之后。它通常由两个线性变换和一个 ReLU (或 GELU)激活函数组成。其核心作用主要有2点：①增加非线性变换，提升模型表达能力；②对注意力层的输出进行更复杂的特征提取和转换。
* **注意力层中已经有softmax非线性层，那么前馈神经网络是否必要**：有必要。softmax是一个非线性函数，但是它的作用域和目的与FFN中的激活函数完全不同。前馈神经网络在Transformer中是必要的，尽管注意力层已有softmax非线性，但：
  - 功能互补：注意力层处理位置间关系，FFN处理位置内特征
  - 数学操作不同：FFN提供更丰富的非线性变换能力
  - 实践验证：移除FFN会显著降低模型性能
  - 参数分布：FFN占模型大部分参数，承担重要的特征变换任务

|特征|注意力层（softmax）|前馈神经网络（FNN）|
|----|----|----|
|**主要目的**|计算注意力权重，用于**信息融合**|进行非线性**特征转换**，增加模型深度|
|**操作范围**|跨整个序列（Sequence-wise）|逐个位置独立（Position-wise）|
|**非线性作用**|归一化 token 间的相关性分数|转换和提炼单个 token 的表示|
|**是否可替代**|否，两者功能完全不同，互为补充|否，移除后模型将退化为近似线性模型|
  
# Q28：如果需要通过修改尽可能少的参数值，让模型忘记某一特定知识，应该修改注意力层还是前馈神经网络层的参数？
前馈神经网络。

1. FFN 是知识的数据库：FFN 层可以被看作是一个巨大的键-值（key-value）存储系统。当模型处理到一个特定的主语或概念（例如，“埃菲尔铁塔位于”）时，FFN中的特定神经元会被激活，并“提取”出与之关联的信息（例如，“巴黎”）。
2. 知识的定位性：研究发现，特定的事实知识往往与 FFN 中一小部分特定的神经元高度相关。这意味着我们可以通过定位并仅修改这些“知识神经元”的权重，来精确地“擦除
      ”或“编辑”某个事实，而对模型的其他能力（如语言流畅性、推理能力等）产生最小的影响。

注意力层的主要作用不是存储知识，而是在上下文中路由和整合信息。
# Q29：大模型在数学计算时，为什么经常不准确？
1. 概率性本质：LLM本质是“语言模型”，而不是“计算模型”。
2. 缺乏真正的数据逻辑（符号处理能力）：模型没有内置的算术逻辑单元（ALU），无法像计算机CPU或计算器那样，按照规则（如进位、借位）去一步步执行计算。它处理数字时，更像是把它们当作普通的文字符号来对待。
3. tokenization的影响:数字会被拆分为更小的单元（Tokens）。例如，“523189” 可能会被拆分成 523 和 189 两个tokens。这种拆分方式不利于模型理解数字的位值（place value），从而在计算时产生错误。

应对方式：
现在先进的大模型通常采用一种“混合模式”来解决这个问题：
* 工具调用：当模型识别到它正在处理一个数学计算或需要精确执行代码的任务时，它不会自己去“猜”答案，而是会调用外部的工具，比如一个代码解释器（如Python）或计算器。
* 结果整合：模型将计算任务交给工具，等待工具返回精确的结果，然后将这个结果整合到它生成的自然语言回答中。

# Q30：模型深度（层数）与宽度（隐藏维度大小）​、注意力头数量、上下文长度等参数之间是如何相互影响的？如果要训练一个比当前模型参数规模大10倍的模型，你会如何调整这些参数？
* 模型深度（层数）与宽度（隐藏维度大小）​、注意力头数量、上下文长度等参数之间是如何相互影响的？
  * 深度 vs 宽度：
    * 深度（层数）：影响模型的计算复杂度和表征能力
    * 宽度（隐藏维度）：影响模型的并行计算能力和参数数量
    * 增加深度往往比增加宽度更有效，但过深会导致梯度消失的问题。
  * 宽度 vs 注意力头数量：两者强相关。增加宽度后，通常必须增加头的数量，以保持每个头的维度 d_head 不变或在一个有效区间内。
    * 通常 隐藏维度 = 头数 × 每头维度
    * 多头注意力提供不同的表征子空间
    * 头数增加可以提高模型容量，但会带来计算开销
  * 上下文长度 vs 其他：上下文长度主要受限于计算资源（特别是内存），而不是参数量。一个拥有巨大参数量的模型如果上下文长度很短，其能力也会受限，反之亦然。
* 如果要训练一个比当前模型参数规模大10倍的模型，你会如何调整这些参数？
  * 策略：优先扩展宽度，其次是深度，并相应调整头数量。
  * 假设我们有一个基线模型（Baseline Model），我们要设计一个参数量为其10倍的新模型（Scaled Model）。这需要遵循“平衡扩展”和“缩放法则”（Scaling Laws）的原则。缩放法则核心思想：模型性能的提升不仅取决于参数量，还取决于训练数据的规模。为了最高效地利用计算资源，模型大小和训练数据量应该按比例增加。在这里，我们只关注模型参数的调整。
  * 因为参数量主要由宽度（d_model^2）和深度（L）决定，即 Params ∝ L * d_model^2。要实现10倍的增长，我们可以有多种组合。一个比较均衡和被证实有效的方法是：
    * 将宽度（`d_model`）增加约2倍。
    * 将深度（`L`）增加约2.5倍。

  这样，总参数量的增长倍数大约是 2.5 * (2^2) = 2.5 * 4 = 10 倍。
# Q31：以一个你熟悉的开源模型为例，介绍模型中每个矩阵的大小和形状。
下面我将以GPT2为例，介绍模型中每个矩阵的大小和形状。
1. 模型基础参数
    - 词汇表大小 (vocab_size): 50,257
    - 上下文长度 (context_length): 1,024
    - 隐藏层维度 (d_model): 1,024
    - 注意力头数 (n_heads): 16
    - 前馈网络维度 (d_ff): 4,096
    - 层数 (n_layers): 24
2. 嵌入层矩阵
     * 词嵌入矩阵 (Token Embedding):①形状: [50257, 1024]；②作用: 将token ID映射到1024维向量空间
     * 位置编码矩阵 (Positional Embedding):①形状: [1024, 1024]；②作用: 为每个位置提供位置信息
3. 多头注意力机制矩阵。每个注意力头有4个权重矩阵：
    * 查询矩阵 (Q):①形状: [1024, 64] (d_model/n_heads = 1024/16 = 64)；②总参数量: 16个头的Q矩阵 = [1024, 1024]
    * 键矩阵 (K):①形状: [1024, 64]；②总参数量: [1024, 1024]
    * 值矩阵 (V):①形状: [1024, 64]；②总参数量: [1024, 1024]
    * 输出投影矩阵 (O):①形状: [1024, 1024]；②作用: 将多头注意力输出投影回d_model维度
4. 前馈网络矩阵。每个Transformer层包含一个两层的前馈网络：
    * 第一层线性变换 (FFN Up):①形状: [1024, 4096]；②作用: 将维度从1024扩展到4096
    * 第二层线性变换 (FFN Down):①形状: [4096, 1024]；②作用: 将维度从4096压缩回1024
5. 层归一化参数。每个Transformer层有2个层归一化：
    * Attention前层归一化: 2个参数向量 [1024] (gamma和beta)
    * FFN前层归一化: 2个参数向量 `[1024]
6. 输出层矩阵
   * 最终层归一化:参数: 2个向量 [1024]
   * 语言模型头 (LM Head):①形状: [1024, 50257]；②作用: 将隐藏状态映射回词汇表概率分布

* 参数统计示例 (GPT-2 Medium)
  - 总参数量: ~355 million
  - 每层注意力参数: 4 × 1024 × 1024 = ~4.2M
  - 每层FFN参数: 1024 × 4096 + 4096 × 1024 = ~8.4M
  - 24层总参数: 24 × (4.2M + 8.4M) = ~302M
  - 嵌入层参数: 50257 × 1024 + 1024 × 1024 = ~52.5M
  - 其他参数: ~0.5
* 关键特点
  1. 参数共享: 所有Transformer层使用相同的矩阵形状
  2. 维度一致性: 输入输出维度保持1024不变
  3. 头维度: 每个注意力头的维度是总维度除以头数
  4. 扩展比: FFN层的扩展比通常是4:1 (1024→4096→1024)

这种矩阵结构设计确保了模型能够有效处理序列数据，同时保持计算效率和表达能力的平衡。

# Q32：大模型推理过程中，内存带宽和算力哪个是瓶颈？以一个你熟悉的开源模型为例，计算输入批次大小达到多少时，能够平衡利用内存带宽和算力？
在大模型推理过程中，内存带宽通常是主要瓶颈，特别是在小批次推理时。 内存带宽是大模型推理的主要瓶颈，理论上，输入批次大小（Batch Size）需要达到 约 156 时，A100 GPU在处理Llama 2 7B模型的一个解码步骤时，其计算时间和内存访问时间才能大致相等。

LLM推理几乎总是带宽瓶颈的。优化方向也主要集中在如何减少内存访问，例如使用更高效的Attention机制（如FlashAttention）、量化（Quantization）和模型剪枝（Pruning）等技术。
# Q33：从统计学角度看，Transformer输出层假设词元符合什么分布？
分类分布/多项式分布。具体流程是：

1. 生成 Logits：Transformer最后的线性层会为词汇表中的每一个词元都生成一个原始的、未归一化的分数，这个分数向量通常被称为“logits”。
2. 应用softmax 函数：接着，softmax 函数会作用在这个 logits 向量上。
3. 转换为概率分布：softmax函数将logits向量转换为一个概率分布。这个分布的特性是：①每个词元的概率值都在0和1之间；②所有词元的概率值加起来总和为1。

这个最终生成的、覆盖整个词汇表的概率集合，就是一个分类分布。模型基于这个分布来预测下一个最有可能出现的词元。在贪心解码（Greedy Decoding）策略中，模型会直接选择概率最高的那个词元作为输出。

* 输出层结构：Transformer输出层通常是一个线性变换+softmax激活函数。
* 概率分布：softmax函数将logits转换为概率分布，使得所有词汇表上的概率和为1。
# Q34：给定一个支持8K上下文的开源模型，如何把它扩展成支持32K上下文的模型？上下文长度增加后对KV缓存会带来什么挑战？

# Q35：为什么注意力机制需要多个头？GQA、MQA优化跟简单减少注意力头的数量相比，有什么不同？GQA、MQA优化的是训练阶段还是推理阶段？
1. **为什么注意力机制需要多个头（Multi-Head Attention）**？
   * 并行处理：将原始的查询（Query）、键（Key）、值（Value）矩阵通过不同的线性变换（WQ, WK, WV 矩阵）投影到多个独立的、更低维度的“表示子空间”（representation subspace）中。每个注意力头可以专注于捕捉不同类型的依赖关系。
   * 增强模型表达能力/信息整合：多个头相当于多个"专家"，每个专家从不同角度分析输入，最后综合结果。
  
   简单来说，多头注意力机制允许模型在不同位置、从不同“表示子空间”关注信息。结论：多头让模型能够同时捕捉输入序列中多种不同类型和维度的关联信息，从而获得更强大、更全面的特征表示能力。

2. **GQA(Grouped-Query Attention)、MQA(Multi-Query Attention)优化跟简单减少注意力头的数量相比，有什么不同？**
   
MHA（多头注意力）虽然效果好，但有一个巨大的性能瓶颈，尤其是在推理（inference）阶段：KV Cache。在生成文本时，模型每生成一个新词，都需要回顾之前所有词的 Key 和 Value。这个存储着历史 K/V 的缓存（KV Cache）会变得非常大，反复从内存（HBM）读取 KV Cache到计算核心（SRAM）会消耗大量的时间和带宽。
    
       * 简单减少头的数量：
             * 做法：比如直接将一个 32 头的模型改成 8 头。
             * 影响：这确实会减少 KV Cache 的大小，因为你同时减少了查询（Q）、键（K）和值（V）头的数量。
             * 缺点：这是“伤筋动骨”的。你直接削减了模型的“专家团队”规模，导致模型的表示能力和性能（模型质量）显著下降。
      
      
       * MQA (Multi-Query Attention)：
           * 做法：保留所有的查询头（Query heads），但让它们共享同一组键（Key）和值（Value）头。例如，一个有 32 个 Q 头的模型，只使用 1 个 K 头和 1 个 V 头。
           * 影响：KV Cache 的大小急剧减小（在这个例子中减小了 32 倍！），因为只需要为单个 K/V 头存储缓存。这极大地降低了内存带宽的压力。
           * 优点：在大幅提升推理速度的同时，尽可能保留 Q 头的多样性。
           * 缺点：所有 Q 头被迫使用同一套 K/V，这可能会损失一些模型质量，因为不同 Q 头无法从不同的 K/V “视角”提取信息。
      
      
       * GQA (Grouped-Query Attention)：
           * 做法：这是 MHA 和 MQA 之间的一个折中方案。它将查询头（Q heads）分组，每个组共享一组 K/V 头。
           * 例子：一个有 32 个 Q 头的模型，可以分成 4 组，每组 8 个 Q 头，每组共享 1 个 K/V 头。这样总共就有 4 个 K/V 头。
           * 影响：KV Cache 的大小显著减小（在这个例子中是 8 倍），但又不像 MQA 那样极端。
           * 优点：它在 MHA 的高质量和 MQA 的高速度之间取得了绝佳的平衡。它比 MHA 快得多，同时因为保留了多组 K/V 头，其模型质量通常比 MQA 更高。
      
      核心区别总结：
       * 简单减头：同时减少 Q, K, V 的数量，直接降低模型容量。
       * MQA/GQA：只减少 K 和 V 头的数量，保留全部或大部分 Q 头的多样性。它们的目标是在不严重损害模型质量的前提下，专门优化 KV Cache 的大小和读取效率。

3. **GQA、MQA优化的是训练阶段还是推理阶段？**

主要优化的是推理（Inference）阶段，尤其是对于长序列的自回归生成任务。

   * 推理阶段：这是 GQA/MQA 发挥最大作用的地方。如前所述，生成每个新 token 都需要访问整个 KV Cache。GQA/MQA 通过减小 KV Cache
     的体积，显著降低了内存带宽需求，从而大幅加快了生成速度并减少了显存占用。这就是为什么像 Llama 2/3、Mixtral 等模型都采用 GQA 的原因。

   * 训练阶段：GQA/MQA 也能带来一些好处，比如减少了训练时的显存占用，可能允许使用更大的批次大小（batch size）。但训练时通常是并行计算所有 token
     的注意力，不像推理时那样是串行的“token by token”，所以 KV Cache 带来的瓶颈不如推理时那么突出。

  总而言之，GQA 和 MQA 是为了解决 MHA 在推理时因巨大的 KV Cache 而导致的内存带宽瓶颈问题而设计的精妙优化，它们在速度和质量之间做出了比“简单减少头数”更优的权衡。
   
# Q36：Flash Attention并不能减少计算量，为什么能实现加速？Flash Attention是如何实现增量计算softmax的？
1. Flash Attention并不能减少计算量，为什么能实现加速？
   它通过优化GPU共享内存（GPU's shared memory,SRAM）和高带宽内存HBM（High Brandwidth Memory）之间的数据加载和迁移来加速注意力计算。内存访问机制优化如下：

   传统Attention的问题：
      - 需要将完整的 QK^T（大小为 N*N，N是序列长度）矩阵存储在 HBM（高带宽内存）中
      - 然后计算 softmax，再将结果写回 HBM
      - 最后进行 V 矩阵乘法，再次读写 HBM
      - 产生了大量的内存读写操作（O(N²)）

   Flash Attention 的优化：
      - 使用分块计算（tiling）技术。将输入Q, K, V矩阵分成一个个小块（Tile/Block）。
      - 将计算分解为小块，在快速的 SRAM（片上内存）中进行
      - 避免了频繁的 HBM 访问
      - 内存访问复杂度从 O(N²) 降到 O(N)

2. Flash Attention是如何实现增量计算softmax的？
   
   标准的Softmax需要知道一个向量中的所有元素才能计算归一化因子（分母）。但在分块计算时，我们一次只能看到一部分元素。 Flash Attention采用了一种在线（Online）的Softmax计算方法。它在处理每个块时，会维护和更新三个关键的统计量：
   1. 行最大值（Row Maximum `m`）：到目前为止，当前行（对应一个Query）计算过的所有QK^T得分中的最大值。
   2. 归一化分母（Normalization Factor `l`）：到目前为止，当前行Softmax分母的累加值。
   3. 输出结果（Output `O`）：到目前为止，当前行Attention的加权平均输出。

   当计算一个新的块时，算法步骤如下：
      1. 在SRAM中计算当前Q块和K块的点积，得到一个新的得分块。
      2. 找到这个新块中的最大值，并与之前保存的行最大值m比较，更新为新的全局最大值m_new。
      3. 重新缩放（Rescale）：如果m_new比旧的m大，就需要用一个缩放因子 exp(m - m_new) 来“修正”之前已经计算好的输出O和分母l。这是为了保证数值稳定性，防止指数计算溢出。
      4. 计算新块的Softmax值（使用新的最大值m_new进行缩放）。
      5. 将新块的计算结果（新的分母和新的加权V值）与之前缩放过的旧结果l和O进行合并，得到更新后的输出。 

   通过这种方式，Flash Attention可以在不访问完整QK^T矩阵的情况下，一块一块地、迭代地计算出与标准Softmax完全相同的结果，同时保持了数值的稳定性。
# Q37：RoPE（旋转位置嵌入）相比Transformer论文中的绝对位置编码有什么优点？RoPE在长上下文外推时会面临什么挑战？
1. RoPE（旋转位置嵌入）相比Transformer论文中的绝对位置编码有什么优点？
   1. 相对位置编码：RoPE通过旋转矩阵对query和key进行变换，编码了相对位置信息而非绝对位置
   2. 更好的外推能力：理论上支持长度外推，因为旋转操作是周期性的
   3. 数学优雅性：基于复数旋转的数学理论基础更坚实
   4. 计算效率：可以在attention计算前预先计算旋转矩阵，减少计算开销

   RoPE 是应用在注意力步骤中的，而不是应用在前向传播的开始。与在前向传播开始时添加的静态绝对嵌入不同，旋转位置嵌入是一种以捕获绝对和相对词元位置信息的方式来编码位置信息的方法，其思想的基础是嵌入空间中旋转的向量。
   > 1. 绝对位置编码（Absolute Positional Encoding, APE）是通过一个固定的 sin 和 cos 函数表，为每个位置生成一个唯一的向量，然后将其加到词嵌入向量上。
   > 2. RoPE（Rotary Positional Embedding，旋转位置嵌入）则另辟蹊径，它不直接修改词嵌入，而是在计算 Query (Q) 和 Key (K) 的注意力得分之前，对它们进行旋转操作。
2. RoPE在长上下文外推时会面临什么挑战？
   1. 周期性问题：旋转操作具有周期性，当序列长度超过旋转周期时，不同位置可能产生相同的编码，导致位置混淆
   2. 高频信息丢失：对于非常长的序列，高频位置信息可能逐渐衰减
   3. 外推性能下降：虽然理论上支持外推，但实际应用中性能会随着序列长度增加而下降
   4. 训练-推理不一致：模型在训练时看到的序列长度有限，外推到更长序列时可能表现不佳
   5. 注意力模式变化：长序列可能导致注意力分布发生不可预测的变化
# Q38：由于训练样本长度往往小于最大上下文长度，把多个训练样本放到同一个上下文中训练时，如何避免它们互相干扰？
打包（序列打包或样本打包）。核心思想是在一个序列中放入多个独立的样本，并使用特定机制来防止它们在计算注意力和损失的时候互相干扰。主要通过以下两种关键技术实现：

1. 注意力掩码
2. 损失掩码
3. 辅助技术：特殊分隔符

通过注意力掩码和损失掩码的组合，我们可以将多个较短的训练样本“打包”到一个更长的序列中，然后一次性送入模型进行训练。
* 注意力掩码保证了在前向传播时，不同样本的信息不会在注意力层泄露和混合。
* 损失掩码保证了在反向传播时，模型不会因为样本拼接处的人为边界而学到错误的信号。

这种方法极大地提高了训练效率，减少了因填充（Padding）短序列造成的计算浪费，是现代大模型训练中不可或缺的一项关键优化技术。
# Q39：如何利用一个小规模的大模型提升大规模模型的推理性能，并尽量不影响大模型的推理结果？推测解码并没有减少计算量，为什么能提升推理性能？
* 如何利用一个小规模的大模型提升大规模模型的推理性能，并尽量不影响大模型的推理结果？<br>
  推测解码（Speculative Encoding）。推测解码是一种利用小模型辅助大模型提升推理性能的技术。 简单来说，这项技术的核心思想是：用一个计算开销小、速度快的小模型来“预先猜测”一段文本，然后让能力强、但速度慢的大模型一次性“批量验证”这些猜测，从而减少大模型“逐字生成”的次数，实现加速。<br>
    - 小模型先行推测
    - 大模型并行验证
    - 接受/拒绝机制
* 推测解码并没有减少计算量，为什么能提升推理性能？<br>
推测解码的总计算量（FLOPs）甚至可能比传统解码更高（因为小模型也在计算，而且有时会做无用功）。性能提升的关键不在于计算量，而在于内存访问模式和硬件并行效率。LLM 推理的主要瓶颈是内存带宽 (Memory Bandwidth)，而不是纯粹的计算能力。

# 论文
* [A survey of Transformers](https://arxiv.org/pdf/2106.04554)
* [EFFICIENT SEQUENCE PACKING WITHOUT CROSS-CONTAMINATION: ACCELERATING LARGE LANGUAGE MODELS WITHOUT IMPACTING PERFORMANCE](https://arxiv.org/pdf/2107.02027)
* [Flash Attention](https://arxiv.org/pdf/2205.14135)
