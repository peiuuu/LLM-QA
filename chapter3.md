**第3章　LLM的内部机制**

# Q20：大模型怎么知道它的输出该结束了？
大模型知道它的输出该结束了，主要通过以下几种机制。

1. **停止标记**：大模型在训练时会学习特殊的停止标记，这些标记告诉模型输出应该结束。当模型生成回答时，当它预测下一个最应该出现的词是停止标记时，生成过程就自动停止了。
2. **最大长度限制**：模型有预设的最大生成长度限制。当模型的输出达到了这个预设的上限时会自动停止生成。
# Q21：训练时如何防止模型看到未来的词元？
在训练语言模型（尤其是像GPT这样的自回归模型）时，为了防止模型在预测当前词元时“偷看”到未来的词元，最关键的技术是使用注意力掩码（Attention Mask）或因果掩码（Causal Mask）。这个过程发生在 Transformer 模型的自注意力（Self-Attention）层中。

**核心思想**：对于序列中的任何一个词元（token），模型在计算它的表征时，只能关注它自己以及它前面的所有词元，绝对不能关注它后面的词元。

1. **因果掩码（Casual Mask）**：通过下三角矩阵确保模型只能看到当前及之前的信息。
2. **自回归特性**：这种设计使模型逐词生成任务。
# Q22：注意力机制是如何计算上下文各个词元之间的相关性的？每个注意力头是只关注一个词元吗？softmax之前为什么要除以 $\sqrt{d_k}$?
1. **相关性计算**：注意力机制通过查询（Query）、键（Key）、V（Value）三元组来计算词元之间的相关性。注意力机制的核心思想是为序列中的每个词元（token）生成一个加权的上下文表示。这个权重代表了序列中其他所有词元（包括它自己）与当前词元的相关性。这个过程是通过三个向量来实现的：查询（Query, Q）、键（Key, K） 和 值（Value, V）。
   * 查询 (Query, Q): 代表当前正在处理的词元。它是“探寻者”，提出问题：“根据我自己的身份，过去哪些信息对于预测下一个词元最重要？”
   * 键 (Key, K): 代表一个过去词元的“标签”或“标识符”。Q 会与它进行比较，来计算注意力分数。它回答了“这个过去的词元与我（Q）相关吗？”这个问题。
   * 值 (Value, V): 代表一个过去词元的实际“内容”或信息。一旦 Q 找到了相关的 K，与之配对的 V 就会被传递过去，作为合成下一个词元的参考。
3. **注意力头**：每个注意力头不是只关注一个词元，每个头关注整个序列中的所有词元，但会学习不同的关注模式（语法、语义、位置等）。
4. **除以 $\sqrt{d_k}$**：数学上为了控制方差，防止梯度消失，确保训练稳定性。这个操作被称为缩放点积注意力（Scaled Dot-Product Attention）。
# Q23：Q和K在注意力的表达式里看起来是对称的，但KV缓存里为什么只有KV，没有Q？
简单来说：Q 代表“现在”，K 和 V 代表“过去”。 在生成文本时，我们只需要缓存“过去”的信息。缓存旧的Q向量是无用的，因为你永远不需要从过去的位置再次提出相同的问题。你永远是从当前位置，对整个过去提问。因此，虽然 Q 和 K 在数学公式上是对称的，但它们在语言生成这个具有时间依赖性的因果过程中，所扮演的角色是完全不对称的。KV缓存正是利用了这种操作上的不对称性，通过避免重复计算过去的状态，极大地提升了生成效率。

| 组件        | 生成过程中的角色           | 为什么被缓存（或不被缓存）                                                                 |
|-------------|----------------------------|--------------------------------------------------------------------------------------------|
| **键 (K)**  | 代表一个**过去**的词元     | **需要缓存**：因为“现在”的词元需要和所有“过去”的词元进行比较，以计算相关性。               |
| **值 (V)**  | 代表一个**过去**词元的内容 | **需要缓存**：因为这是“现在”的词元最终要从“过去”提取的实际信息。                         |
| **查询 (Q)**| 代表**当前**的词元，正在提问 | **无需缓存**：它非常短暂，只在当前步使用。下一步会生成一个全新的 Q，旧的 Q 就没用了。     |

 虽然Q和K在数学表达式上是对称的，但在计算图和内存使用模式上存在本质差异：
  - Q：每个时间步都需要重新计算，是瞬态的
  - K和V：一旦计算就可以被后续所有时间步复用，是持久的
    
 这种设计使得KV缓存能够显著提高推理效率，减少重复计算，同时保持最小的内存开销。
# Q24：如果没有KV缓存，推理性能会降低多少？
没有KV缓存会导致推理性能呈二次方下降，从O(L)变为O(L²)。

# Q25：为什么Transformer中需要残差连接？
**残差连接**：就是将一个模块的输入直接加到这个模块的输出上。用公式表示就是：Output = Layer(x) + x
   * 其中 x 是输入，Layer(x) 是一个或多个网络层（比如自注意力层或前馈网络层）对 x 的处理结果。
     
引入残差连接主要是为了解决深度神经网络中普遍存在的两个核心问题：
   * 解决梯度消失的问题
   * 避免网络退化的问题

在 Transformer 中使用残差连接有以下核心好处：
   * 解决梯度消失，让梯度能够有效地在深层网络中传播。
   * 允许构建非常深的网络，这是 Transformer 模型能够堆叠多个编码器和解码器层的关键。
   * 加速模型训练，因为梯度流更顺畅，收敛更快。
   * 保留原始信息，将输入 x 的信息直接传递到下一层，避免在多层转换中丢失关键特征。

# Q26：Transformer中的LayerNorm跟ResNet中的BatchNorm有什么区别，为什么Llama-3换用了RMSNorm？
简单来说，这三者是归一化技术发展的不同阶段，核心目标都是为了让模型训练更稳定、更快速，但它们的策略和计算效率不同。
   - BatchNorm (BN)：元老级的归一化方法，为CNN设计。
   - LayerNorm (LN)：为解决BN在RNN和Transformer上的水土不服而生。
   - RMSNorm（Root Mean Square Normalization，均方根归一化）：LayerNorm的“减配提速”版，在现代LLM中表现优异，被Llama-3等模型采用。

1. Transformer中的LayerNorm跟ResNet中的BatchNorm有什么区别？从 BatchNorm 到 LayerNorm 是为了适应模型架构（从CNN到Transformer）的必要变革。
2. 为什么Llama-3换用了RMSNorm？从 LayerNorm 到 RMSNorm 则是大模型时代追求极致效率的优化选择。Llama-3采用RMSNorm，是在保证模型性能不下降的前提下，通过简化计算来大幅提升训练和推理速度，这对于动辄需要数千块GPU训练数周的SOTA模型来说，是至关重要的。
# Q27：Transformer中前馈神经网络的作用是什么？注意力层中已经有softmax非线性层，那么前馈神经网络是否必要？
* **Transformer中前馈神经网络（Feed-Forward Network, FFN）的作用**：记忆和插值。模型能够利用相同的机制在数据点之间进行插值，识别更复杂的模式，从而实现泛化。在 Transformer 的每个编码器（Encoder）和解码器（Decoder）层中，FFN 都位于自注意力（Self-Attention）层之后。它通常由两个线性变换和一个 ReLU (或 GELU)激活函数组成。其核心作用主要有2点：①增加非线性变换，提升模型表达能力；②对注意力层的输出进行更复杂的特征提取和转换。
* **注意力层中已经有softmax非线性层，那么前馈神经网络是否必要**：有必要。softmax是一个非线性函数，但是它的作用域和目的与FFN中的激活函数完全不同。前馈神经网络在Transformer中是必要的，尽管注意力层已有softmax非线性，但：
  - 功能互补：注意力层处理位置间关系，FFN处理位置内特征
  - 数学操作不同：FFN提供更丰富的非线性变换能力
  - 实践验证：移除FFN会显著降低模型性能
  - 参数分布：FFN占模型大部分参数，承担重要的特征变换任务

|特征|注意力层（softmax）|前馈神经网络（FNN）|
|----|----|----|
|**主要目的**|计算注意力权重，用于**信息融合**|进行非线性**特征转换**，增加模型深度|
|**操作范围**|跨整个序列（Sequence-wise）|逐个位置独立（Position-wise）|
|**非线性作用**|归一化 token 间的相关性分数|转换和提炼单个 token 的表示|
|**是否可替代**|否，两者功能完全不同，互为补充|否，移除后模型将退化为近似线性模型|
  
# Q28：如果需要通过修改尽可能少的参数值，让模型忘记某一特定知识，应该修改注意力层还是前馈神经网络层的参数？
前馈神经网络。

1. FFN 是知识的数据库：FFN 层可以被看作是一个巨大的键-值（key-value）存储系统。当模型处理到一个特定的主语或概念（例如，“埃菲尔铁塔位于”）时，FFN中的特定神经元会被激活，并“提取”出与之关联的信息（例如，“巴黎”）。
2. 知识的定位性：研究发现，特定的事实知识往往与 FFN 中一小部分特定的神经元高度相关。这意味着我们可以通过定位并仅修改这些“知识神经元”的权重，来精确地“擦除
      ”或“编辑”某个事实，而对模型的其他能力（如语言流畅性、推理能力等）产生最小的影响。

注意力层的主要作用不是存储知识，而是在上下文中路由和整合信息。
# Q29：大模型在数学计算时，为什么经常不准确？
1. 概率性本质：LLM本质是“语言模型”，而不是“计算模型”。
2. 缺乏真正的数据逻辑（符号处理能力）：模型没有内置的算术逻辑单元（ALU），无法像计算机CPU或计算器那样，按照规则（如进位、借位）去一步步执行计算。它处理数字时，更像是把它们当作普通的文字符号来对待。
3. tokenization的影响:数字会被拆分为更小的单元（Tokens）。例如，“523189” 可能会被拆分成 523 和 189 两个tokens。这种拆分方式不利于模型理解数字的位值（place value），从而在计算时产生错误。

应对方式：
现在先进的大模型通常采用一种“混合模式”来解决这个问题：
* 工具调用：当模型识别到它正在处理一个数学计算或需要精确执行代码的任务时，它不会自己去“猜”答案，而是会调用外部的工具，比如一个代码解释器（如Python）或计算器。
* 结果整合：模型将计算任务交给工具，等待工具返回精确的结果，然后将这个结果整合到它生成的自然语言回答中。

# Q30：模型深度（层数）与宽度（隐藏维度大小）​、注意力头数量、上下文长度等参数之间是如何相互影响的？如果要训练一个比当前模型参数规模大10倍的模型，你会如何调整这些参数？
* 模型深度（层数）与宽度（隐藏维度大小）​、注意力头数量、上下文长度等参数之间是如何相互影响的？
  * 深度 vs 宽度：
    * 深度（层数）：影响模型的计算复杂度和表征能力
    * 宽度（隐藏维度）：影响模型的并行计算能力和参数数量
    * 增加深度往往比增加宽度更有效，但过深会导致梯度消失的问题。
  * 宽度 vs 注意力头数量：两者强相关。增加宽度后，通常必须增加头的数量，以保持每个头的维度 d_head 不变或在一个有效区间内。
    * 通常 隐藏维度 = 头数 × 每头维度
    * 多头注意力提供不同的表征子空间
    * 头数增加可以提高模型容量，但会带来计算开销
  * 上下文长度 vs 其他：上下文长度主要受限于计算资源（特别是内存），而不是参数量。一个拥有巨大参数量的模型如果上下文长度很短，其能力也会受限，反之亦然。
* 如果要训练一个比当前模型参数规模大10倍的模型，你会如何调整这些参数？
  * 策略：优先扩展宽度，其次是深度，并相应调整头数量。
  * 假设我们有一个基线模型（Baseline Model），我们要设计一个参数量为其10倍的新模型（Scaled Model）。这需要遵循“平衡扩展”和“缩放法则”（Scaling Laws）的原则。缩放法则核心思想：模型性能的提升不仅取决于参数量，还取决于训练数据的规模。为了最高效地利用计算资源，模型大小和训练数据量应该按比例增加。在这里，我们只关注模型参数的调整。
  * 因为参数量主要由宽度（d_model^2）和深度（L）决定，即 Params ∝ L * d_model^2。要实现10倍的增长，我们可以有多种组合。一个比较均衡和被证实有效的方法是：
    * 将宽度（`d_model`）增加约2倍。
    * 将深度（`L`）增加约2.5倍。

  这样，总参数量的增长倍数大约是 2.5 * (2^2) = 2.5 * 4 = 10 倍。
# Q31：以一个你熟悉的开源模型为例，介绍模型中每个矩阵的大小和形状。
下面我将以GPT2为例，介绍模型中每个矩阵的大小和形状。
1. 模型基础参数
    - 词汇表大小 (vocab_size): 50,257
    - 上下文长度 (context_length): 1,024
    - 隐藏层维度 (d_model): 1,024
    - 注意力头数 (n_heads): 16
    - 前馈网络维度 (d_ff): 4,096
    - 层数 (n_layers): 24
2. 嵌入层矩阵
     * 词嵌入矩阵 (Token Embedding):①形状: [50257, 1024]；②作用: 将token ID映射到1024维向量空间
     * 位置编码矩阵 (Positional Embedding):①形状: [1024, 1024]；②作用: 为每个位置提供位置信息
3. 多头注意力机制矩阵。每个注意力头有4个权重矩阵：
    * 查询矩阵 (Q):①形状: [1024, 64] (d_model/n_heads = 1024/16 = 64)；②总参数量: 16个头的Q矩阵 = [1024, 1024]
    * 键矩阵 (K):①形状: [1024, 64]；②总参数量: [1024, 1024]
    * 值矩阵 (V):①形状: [1024, 64]；②总参数量: [1024, 1024]
    * 输出投影矩阵 (O):①形状: [1024, 1024]；②作用: 将多头注意力输出投影回d_model维度
4. 前馈网络矩阵。每个Transformer层包含一个两层的前馈网络：
    * 第一层线性变换 (FFN Up):①形状: [1024, 4096]；②作用: 将维度从1024扩展到4096
    * 第二层线性变换 (FFN Down):①形状: [4096, 1024]；②作用: 将维度从4096压缩回1024
5. 层归一化参数。每个Transformer层有2个层归一化：
    * Attention前层归一化: 2个参数向量 [1024] (gamma和beta)
    * FFN前层归一化: 2个参数向量 `[1024]
6. 输出层矩阵
   * 最终层归一化:参数: 2个向量 [1024]
   * 语言模型头 (LM Head):①形状: [1024, 50257]；②作用: 将隐藏状态映射回词汇表概率分布

* 参数统计示例 (GPT-2 Medium)
  - 总参数量: ~355 million
  - 每层注意力参数: 4 × 1024 × 1024 = ~4.2M
  - 每层FFN参数: 1024 × 4096 + 4096 × 1024 = ~8.4M
  - 24层总参数: 24 × (4.2M + 8.4M) = ~302M
  - 嵌入层参数: 50257 × 1024 + 1024 × 1024 = ~52.5M
  - 其他参数: ~0.5
* 关键特点
  1. 参数共享: 所有Transformer层使用相同的矩阵形状
  2. 维度一致性: 输入输出维度保持1024不变
  3. 头维度: 每个注意力头的维度是总维度除以头数
  4. 扩展比: FFN层的扩展比通常是4:1 (1024→4096→1024)

这种矩阵结构设计确保了模型能够有效处理序列数据，同时保持计算效率和表达能力的平衡。

# Q32：大模型推理过程中，内存带宽和算力哪个是瓶颈？以一个你熟悉的开源模型为例，计算输入批次大小达到多少时，能够平衡利用内存带宽和算力？
在大模型推理过程中，内存带宽通常是主要瓶颈，特别是在小批次推理时。 内存带宽是大模型推理的主要瓶颈，理论上，输入批次大小（Batch Size）需要达到 约 156 时，A100 GPU在处理Llama 2 7B模型的一个解码步骤时，其计算时间和内存访问时间才能大致相等。

LLM推理几乎总是带宽瓶颈的。优化方向也主要集中在如何减少内存访问，例如使用更高效的Attention机制（如FlashAttention）、量化（Quantization）和模型剪枝（Pruning）等技术。
# Q33：从统计学角度看，Transformer输出层假设词元符合什么分布？
分类分布/多项式分布。具体流程是：

1. 生成 Logits：Transformer最后的线性层会为词汇表中的每一个词元都生成一个原始的、未归一化的分数，这个分数向量通常被称为“logits”。
2. 应用softmax 函数：接着，softmax 函数会作用在这个 logits 向量上。
3. 转换为概率分布：softmax函数将logits向量转换为一个概率分布。这个分布的特性是：①每个词元的概率值都在0和1之间；②所有词元的概率值加起来总和为1。

这个最终生成的、覆盖整个词汇表的概率集合，就是一个分类分布。模型基于这个分布来预测下一个最有可能出现的词元。在贪心解码（Greedy Decoding）策略中，模型会直接选择概率最高的那个词元作为输出。

* 输出层结构：Transformer输出层通常是一个线性变换+softmax激活函数。
* 概率分布：softmax函数将logits转换为概率分布，使得所有词汇表上的概率和为1。
# Q34：给定一个支持8K上下文的开源模型，如何把它扩展成支持32K上下文的模型？上下文长度增加后对KV缓存会带来什么挑战？

# Q35：为什么注意力机制需要多个头？GQA、MQA优化跟简单减少注意力头的数量相比，有什么不同？GQA、MQA优化的是训练阶段还是推理阶段？
1. **为什么注意力机制需要多个头（Multi-Head Attention）**？
   * 并行处理：将原始的查询（Query）、键（Key）、值（Value）矩阵通过不同的线性变换（WQ, WK, WV 矩阵）投影到多个独立的、更低维度的“表示子空间”（representation subspace）中。每个注意力头可以专注于捕捉不同类型的依赖关系。
   * 增强模型表达能力/信息整合：多个头相当于多个"专家"，每个专家从不同角度分析输入，最后综合结果。
  
   简单来说，多头注意力机制允许模型在不同位置、从不同“表示子空间”关注信息。结论：多头让模型能够同时捕捉输入序列中多种不同类型和维度的关联信息，从而获得更强大、更全面的特征表示能力。

2. **GQA(Grouped-Query Attention)、MQA(Multi-Query Attention)优化跟简单减少注意力头的数量相比，有什么不同？**
   
MHA（多头注意力）虽然效果好，但有一个巨大的性能瓶颈，尤其是在推理（inference）阶段：KV Cache。在生成文本时，模型每生成一个新词，都需要回顾之前所有词的 Key 和 Value。这个存储着历史 K/V 的缓存（KV Cache）会变得非常大，反复从内存（HBM）读取 KV Cache到计算核心（SRAM）会消耗大量的时间和带宽。
    
       * 简单减少头的数量：
             * 做法：比如直接将一个 32 头的模型改成 8 头。
             * 影响：这确实会减少 KV Cache 的大小，因为你同时减少了查询（Q）、键（K）和值（V）头的数量。
             * 缺点：这是“伤筋动骨”的。你直接削减了模型的“专家团队”规模，导致模型的表示能力和性能（模型质量）显著下降。
      
      
       * MQA (Multi-Query Attention)：
           * 做法：保留所有的查询头（Query heads），但让它们共享同一组键（Key）和值（Value）头。例如，一个有 32 个 Q 头的模型，只使用 1 个 K 头和 1 个 V 头。
           * 影响：KV Cache 的大小急剧减小（在这个例子中减小了 32 倍！），因为只需要为单个 K/V 头存储缓存。这极大地降低了内存带宽的压力。
           * 优点：在大幅提升推理速度的同时，尽可能保留 Q 头的多样性。
           * 缺点：所有 Q 头被迫使用同一套 K/V，这可能会损失一些模型质量，因为不同 Q 头无法从不同的 K/V “视角”提取信息。
      
      
       * GQA (Grouped-Query Attention)：
           * 做法：这是 MHA 和 MQA 之间的一个折中方案。它将查询头（Q heads）分组，每个组共享一组 K/V 头。
           * 例子：一个有 32 个 Q 头的模型，可以分成 4 组，每组 8 个 Q 头，每组共享 1 个 K/V 头。这样总共就有 4 个 K/V 头。
           * 影响：KV Cache 的大小显著减小（在这个例子中是 8 倍），但又不像 MQA 那样极端。
           * 优点：它在 MHA 的高质量和 MQA 的高速度之间取得了绝佳的平衡。它比 MHA 快得多，同时因为保留了多组 K/V 头，其模型质量通常比 MQA 更高。
      
      核心区别总结：
       * 简单减头：同时减少 Q, K, V 的数量，直接降低模型容量。
       * MQA/GQA：只减少 K 和 V 头的数量，保留全部或大部分 Q 头的多样性。它们的目标是在不严重损害模型质量的前提下，专门优化 KV Cache 的大小和读取效率。

3. **GQA、MQA优化的是训练阶段还是推理阶段？**

主要优化的是推理（Inference）阶段，尤其是对于长序列的自回归生成任务。

   * 推理阶段：这是 GQA/MQA 发挥最大作用的地方。如前所述，生成每个新 token 都需要访问整个 KV Cache。GQA/MQA 通过减小 KV Cache
     的体积，显著降低了内存带宽需求，从而大幅加快了生成速度并减少了显存占用。这就是为什么像 Llama 2/3、Mixtral 等模型都采用 GQA 的原因。

   * 训练阶段：GQA/MQA 也能带来一些好处，比如减少了训练时的显存占用，可能允许使用更大的批次大小（batch size）。但训练时通常是并行计算所有 token
     的注意力，不像推理时那样是串行的“token by token”，所以 KV Cache 带来的瓶颈不如推理时那么突出。

  总而言之，GQA 和 MQA 是为了解决 MHA 在推理时因巨大的 KV Cache 而导致的内存带宽瓶颈问题而设计的精妙优化，它们在速度和质量之间做出了比“简单减少头数”更优的权衡。
   
# Q36：Flash Attention并不能减少计算量，为什么能实现加速？Flash Attention是如何实现增量计算softmax的？

# Q37：RoPE（旋转位置嵌入）相比Transformer论文中的绝对位置编码有什么优点？RoPE在长上下文外推时会面临什么挑战？

# Q38：由于训练样本长度往往小于最大上下文长度，把多个训练样本放到同一个上下文中训练时，如何避免它们互相干扰？

# Q39：如何利用一个小规模的大模型提升大规模模型的推理性能，并尽量不影响大模型的推理结果？推测解码并没有减少计算量，为什么能提升推理性能？


