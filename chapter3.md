**第3章　LLM的内部机制**

# Q20：大模型怎么知道它的输出该结束了？
# Q21：训练时如何防止模型看到未来的词元？
# Q22：注意力机制是如何计算上下文各个词元之间的相关性的？每个注意力头是只关注一个词元吗？softmax之前为什么要除以 $\sqrt{dk}$?
# Q23：Q和K在注意力的表达式里看起来是对称的，但KV缓存里为什么只有KV，没有Q？
# Q24：如果没有KV缓存，推理性能会降低多少？
# Q25：为什么Transformer中需要残差连接？
# Q26：Transformer中的LayerNorm跟ResNet中的BatchNorm有什么区别，为什么Llama-3换用了RMSNorm？
# Q27：Transformer中前馈神经网络的作用是什么？注意力层中已经有softmax非线性层，那么前馈神经网络是否必要？
# Q28：如果需要通过修改尽可能少的参数值，让模型忘记某一特定知识，应该修改注意力层还是前馈神经网络层的参数？
# Q29：大模型在数学计算时，为什么经常不准确？
# Q30：模型深度（层数）与宽度（隐藏维度大小）​、注意力头数量、上下文长度等参数之间是如何相互影响的？如果要训练一个比当前模型参数规模大10倍的模型，你会如何调整这些参数？
# Q31：以一个你熟悉的开源模型为例，介绍模型中每个矩阵的大小和形状。
# Q32：大模型推理过程中，内存带宽和算力哪个是瓶颈？以一个你熟悉的开源模型为例，计算输入批次大小达到多少时，能够平衡利用内存带宽和算力？
# Q33：从统计学角度看，Transformer输出层假设词元符合什么分布？
# Q34：给定一个支持8K上下文的开源模型，如何把它扩展成支持32K上下文的模型？上下文长度增加后对KV缓存会带来什么挑战？
# Q35：为什么注意力机制需要多个头？GQA、MQA优化跟简单减少注意力头的数量相比，有什么不同？GQA、MQA优化的是训练阶段还是推理阶段？
# Q36：Flash Attention并不能减少计算量，为什么能实现加速？Flash Attention是如何实现增量计算softmax的？
# Q37：RoPE（旋转位置嵌入）相比Transformer论文中的绝对位置编码有什么优点？RoPE在长上下文外推时会面临什么挑战？
# Q38：由于训练样本长度往往小于最大上下文长度，把多个训练样本放到同一个上下文中训练时，如何避免它们互相干扰？
# Q39：如何利用一个小规模的大模型提升大规模模型的推理性能，并尽量不影响大模型的推理结果？推测解码并没有减少计算量，为什么能提升推理性能？
