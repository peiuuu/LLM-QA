Q1：Transformer中的编码器和解码器有什么区别，只有编码器或者只有解码器的模型是否有用？
Q2：GPT跟原始Transformer论文的模型架构有什么区别？
Q3：仅编码器（BERT类）​、仅解码器（GPT类）和完整编码器-解码器架构各有什么优缺点？
Q4：为什么说Transformer的自注意力机制相对于早期RNN中的注意力机制是一个显著的进步？
Q5：大模型为什么有最长上下文长度的概念？为什么它是指输入和输出的总长度？
Q6：大模型的首字延迟、输入吞吐量、输出吞吐量分别是如何计算的？不同应用场景对首字延迟、输入吞吐量和输出吞吐量的需求分别是什么？
Q7：预训练和微调的两步范式为什么如此重要？基础模型通过预训练获得了哪些核心能力？微调在引导模型遵循指令、回答问题和对齐人类价值观方面起到什么作用？
Q8：Llama-3 8B的综合能力比Llama-1 70B的能力还强，是如何做到的？
