# Q9：大模型的分词器和传统的中文分词有什么区别？对于一个指定的词表，一句话是不是只有一种唯一的分词方式？
  * 传统分词：目标是找到语义上最正确的分词路径，这本身就可能存在多种合理解释。
  * 大模型分词器：目标是确定性地将文本映射为ID序列。因此，对于一个已经训练好的、采用确定性算法（如BPE）的分词器，一句话的分词方式是唯一的。
# Q10：为什么传统BM25检索对中文分词的质量很敏感，而大模型对分词器的选取不敏感？
  * BM25 是基于词袋模型（Bag-of-Words）的词法匹配（Lexical Matching），它只能理解“词”本身，无法理解“词”背后的语义。
  * 大模型（LLM）是基于注意力机制（Attention Mechanism）的语义理解（Semantic Understanding），它在巨大的向量空间中操作，能理解词、字、甚至子词（subword）组合起来的复杂含义。
# Q11：GPT-4、Llama等现代大模型采用的字节级BPE分词器相比传统的BPE分词器有什么优点？
核心优势：通用性与鲁棒性。简单来说，字节级BPE通过在字节（Bytes）层面而不是字符（Characters）层面进行操作，从根本上解决了传统方法的一些限制。总而言之，字节级BPE通过将所有文本都视为原始字节流，从根本上解决了传统分词器面临的词汇表开放性（Out-of-Vocabulary）和多语言支持两大难题。它以一种极为通用和优雅的方式，让模型能够处理任何形式的文本输入，这对于需要消化整个互联网海量、多样化数据的现代大语言模型来说是至关重要的。
1. 彻底消除“未知词元”（Unknown Token / <UNK>)
   * 传统BPE：它的初始词汇表是基于训练语料库中的所有唯一字符（例如，所有英文字母、数字、常见标点符号）。如果模型在推理时遇到一个不在这个初始词汇表里的字符（比如一个罕见的表情符号🤔、一个特殊技术符号⌘或一种新语言的字母ü），它无法处理，只能将其替换为一个统一的“未知”词元（<UNK>）。这会导致信息丢失。
   * 字节级BPE：它的基础词汇表是固定的 256个字节。根据UTF-8编码，地球上任何语言的任何文本、任何符号、甚至乱码，都可以被唯一地表示为一个字节序列。因此，对于字节级BPE来说，不存在“未知”的字符。它总能将任何输入字符串分解成一串字节，从而保留所有信息。
2. 无需预设巨大的字符表，更高效
   * 传统BPE：为了支持多语言，需要一个包含成千上万个字符的巨大初始词汇表（Unicode字符集非常庞大）。这使得分词器的基础构建单元非常臃肿。
   * 字节级BPE：基础词汇表永远是256个字节。模型在此基础上学习频繁出现的字节对组合（比如，代表字母t h
     e的字节序列会很快被合并成一个词元the）。这使得词汇表可以更紧凑、更高效地构建，并且不需要预先“学习”所有语言的字符。
3. 天然无缝支持多语言、代码和特殊格式
   * 传统BPE：通常对特定语言（如英语）效果最好，因为其合并策略是基于该语言的构词规律。处理结构差异巨大的语言（如中文、日文）或代码时，效果可能不佳。
   * 字节级BPE：因为它不关心“字符”或“单词”是什么，只关心字节序列。无论是英文单词、中文汉字、Python代码还是JSON字符串，在它看来都是一串字节流。它能自动学习
     到各种语言和格式中的常见字节组合模式，从而实现真正的语言无关性（Language-Agnostic）。
4. 对噪声和格式变化更鲁棒
   * 网络上的文本充满了各种“噪声”，比如多余的空格、拼写错误、奇怪的Unicode字符、甚至是文本编码错误导致的乱码。
   * 传统BPE：遇到这些噪声时，很容易产生大量的<UNK>词元。
   * 字节级BPE：即使是乱码，也只是一串字节序列。分词器仍然会尽力将其分解和组合，虽然可能不是最优的，但不会像传统方法那样直接“放弃”并丢失信息。这使得模型在
     处理真实世界中不完美的文本数据时表现得更加稳健。
  > BPE(Byte Pair Encoding，字节对编码)。传统的BPE（字节对编码）分词器 vs 现代大模型（如GPT-4、Llama等）采用的字节级BPE（Byte-level BPE）分词器。
  > * GPT-4分词器将四个空白字符表示为单个词元。实际上，它甚至为各种长度的空白字符序列（最多83个）都设有特定的词元。
  > * Python关键字在GPT-4中有自己的词元。这一点和前一点都源于模型对代码和自然语言的关注。
  > * GPT-4分词器使用更少的词元来表示大多数词，例如CAPITALIZATION（用两个词元取代四个词元）和tokens（用一个词元取代三个词元）​。
# Q12：国内预训练的大模型与海外模型相比，是如何做到用相对更少的词元表达中文语料的？
关键原因在于分词器（Tokenizer）的设计和训练方式不同。海外模型（如GPT系列、Llama系列）和中国的大模型（如通义千问、文心一言、Baichuan等）在处理中文时，采用了两种不同的策略：
 1. 海外模型：基于字节的“通用”分词策略
   * 原理：这类模型的目标是处理全球多种语言，它们的词汇表（Vocabulary）主要基于英文构建。当遇到像中文这样的非拉丁语系文字时，由于单个汉字不在它们预设的词汇表中，分词器会退而求其次，将这个汉字按照其计算机存储的编码（通常是UTF-8）进行拆分。
   * 过程：一个UTF-8编码的汉字通常占用3个字节（Bytes）。因此，一个汉字经常被拆分成2到3个更小的单元（Token）。
   * 例子：
       * 对于单词 "Gemini"，它可能被看作1个或2个词元（"Gem" + "ini"）。
       * 对于中文 "你好"，海外模型的分词器可能会把它拆分成类似 ["<e4>","<bd>","<a0>","<e5>","<a5>","<bd>"] 这样的6个字节词元，或者合并成2-4个词元。
   * 缺点：
       * 低效：用多个无实际意义的字节来表示一个有完整意义的汉字，大大增加了处理相同内容所需的词元数量。
       * 语义损失：模型需要先学习如何将这些零散的字节重新组合成一个汉字，然后才能理解其含义，这无疑增加了学习成本和理解难度。

  2. 中国大模型：为中文优化的“专用”分词策略
   * 原理：国内的团队在构建模型之初，就将中文作为最主要的语言进行优化。它们的分词器在构建词汇表时，直接包含了数千个常用汉字以及大量常见的中文词语。
   * 过程：在分词时，每个汉字或一个常见的词语（如“模型”、“中国”）可以直接被映射为词汇表中的一个独立词元（Token）。
   * 例子：
       * 对于中文 "你好"，一个为中文优化的分词器会直接将其拆分为 ["你", "好"] 这2个词元。对于更常见的词，比如“人工智能”，甚至可能直接将其视为1个词元。
   * 优点：
       * 高效：一个汉字就是一个词元，完美匹配了中文的语言特性，极大压缩了文本长度。
       * 语义完整：模型直接在具有完整意义的字和词的层面上进行学习和推理，效率和效果都更好。
       * 更长的有效上下文：在模型有限的上下文窗口（Context Window）内，由于词元效率更高，模型能“读”进更多的中文内容，从而更好地理解长篇文章。

所以，国内大模型之所以能用更少的词元表达中文，是因为它们从设计上就采用了更适合中文语言特性的分词方法，避免了海外模型“水土不服”的低效编码问题。这使得它们在处理中文任务时，无论在成本上还是在性能上都具有天然的优势。
# Q13：大模型是如何区分聊天历史中用户说的话和AI说的话的？
使用特殊的角色标签（Role Lable）。 在将聊天历史输入给模型时，每一段话都会被明确地标记上发送者的角色，比如 user（用户）或model（模型/AI）。所以，模型看到的并不是一段连续的文本，而是一个结构化的对话记录。
举一个简化的例子，模型接收到的内部数据格式可能看起来像这样：
```
[
  {
    "role": "user",
    "content": "你好，请问世界上最高的山是哪座？"
  },
  {
    "role": "model",
    "content": "世界上最高的山是珠穆朗玛峰。"
  },
  {
    "role": "user",
    "content": "它有多高？"
  }
]
```

当模型需要生成新回复时，它会看到上面这个完整的、带有角色标签的列表。因为它看到最后一条消息的角色是 user，所以它知道自己的任务是生成 model角色的下一段回复。
这种方式有几个关键作用：
   1. 提供上下文：模型可以清楚地知道哪句话是谁说的，避免混淆问题和答案。
   2. 学习角色行为：在训练阶段，模型通过海量的类似数据学习到：当看到 user 的提问或陈述后，它应该生成一个作为 model 或 assistant 的、有帮助性的回答。
   3. 控制对话流程：这种结构保证了对话的一问一答形式。
      
所以，总而言之，这并不是一种高级的“意识”，而是通过严格的数据结构和角色标签来实现的。
# Q14：大模型做工具调用的时候，输出的工具调用参数是如何与文本回复区分开来的？
大模型本身输出的本质上由于是一连串的 Token（文本流），区分“普通文本回复”和“工具调用参数”主要依赖于训练时的对齐（Alignment）和推理时的协议（Protocol）。当大模型需要工具调用时，会生成一个特殊格式的、结构化的输出，而不是普通的对话文本。
* 如果是文本回复，大模型会生成一段普通的文本。
* 如果是工具调用，大模型会生成一个特殊的数据块（通常类似于JSON或XML格式）。这个数据块会明确包含：
  * 要调用的工具名称（例如 run_shell_command）。
  * 传递给该工具的参数（例如 command: 'ls -l'）。
# Q15：设计一个使用嵌入技术解决电子商务产品推荐的系统。使用什么数据作为“句子”的等价物？如何将用户行为融入嵌入模型？
* “句子”的最佳等价物是用户的购物会话。 对于prod2vec模型，最理想的“句子”是按时间顺序排列的、在单个会话中用户与之交互（点击、加入购物车、购买）的商品ID序列。这为模型提供了丰富的上下文，让模型可以学习到哪些产品是可替代的（比如用户查看了多个同类产品），哪些是互补的（比如用户购买了相机和存储卡）。
* 将用户行为融入模型有两个主要层次：训练产品嵌入和利用嵌入进行个性化推荐。
  * 通过用户行为训练产品嵌入 (Prod2Vec)
  * 利用嵌入生成个性化推荐
# Q16：word2vec的训练过程中，负例的作用是什么？
负例的作用是大幅提高模型训练的效率和速度。
* 效率提升：将一个需要对几十万个词进行计算的“超级多选题”，变成了一组只有几个选项的“是非判断题”。每次更新时，计算量从 V（整个词汇表大小）下降到了k+1（k个负例 + 1个正例），k 通常是一个很小的数。这使得训练速度提升了成百上千倍。
* 简化目标：模型的目标从“精确计算每个词的概率”转变为“能够区分真实词对和虚假词对”。事实证明，这个简化的目标同样能让模型学习到高质量的词向量。
# Q17：传统的静态词嵌入（如word2vec）与大模型产生的与上下文相关的嵌入相比，有什么区别？有了与上下文相关的嵌入，静态词嵌入还有什么价值？
核心区别：静态 vs 动态。
|特性|静态词嵌入（例word2vec）|上下文相关的嵌入（例BERT）|
|----|----|----|
|**核心**|一词一向量（与语境无关）|一词多向量（由语境动态决定）|
|**优点**|高效、快速、轻量、简单|精准、强大、性能优越|
|**缺点**|无法处理多义词|资源消耗大、模型重、复杂|
|**最适用场景**|效率优先、简单任务、资源受限设备、快速原型和基线|杂NLP任务、追求最高性能、资源充足|

总而言之，上下文相关嵌入是 NLP 领域的技术前沿和发展方向，但静态词嵌入凭借其高效、简单和低成本的特点，在合适的场景下依然是不可或缺的实用工具。技术选型时，应根据具体任务需求、性能目标和资源限制来综合决策。
# Q18：与上下文相关的嵌入是如何解决一词多义问题的，如技术语境下，英文token可能表示词元、代币、令牌，而中文“推理”可能表示reasoning或inference？
embedding 会将词转换为向量，通过不同向量之间的相似度计算来解决一词多义的问题。
# Q19：在word2vec等词嵌入空间中，存在king-man + woman ≈ queen的现象，这是为什么？大模型的词元嵌入空间是否也有类似的属性？
这个现象被称为词向量的线性类比（Linear Analogies），它不是一个巧合，而是 Word2Vec 这类模型学习方式的直接产物。其根本原因在于模型通过学习词语的上下文分布，将语义关系转化为了向量空间中的几何关系。
king - man + woman ≈ queen 的现象是词嵌入将语义关系几何化的体现。这一基本属性在大语言模型的嵌入空间中不仅得以保留，而且变得更加强大和丰富，尽管其上下文相关和子词切分的特性使得这种类比的实现方式更为复杂。
* king-man的几何意义：让我们来分解 king - man + woman 这个运算：
   * `vector('king')`：这个向量代表了“king”这个概念，它包含了“皇室”、“男性”、“权力”、“国家元首”等多种语义特征。
   * `vector('man')`：这个向量代表了“男性”这个概念。
   * `vector('king') - vector('man')`：在向量空间中，两个向量相减，得到的是从一个点指向另一个点的向量。这个新的向量 v_diff = vector('king') - vector('man') 可以被理解为一个“关系向量”。它大致捕获了从“普通男性”到“男性君主”所需要添加的语义概念，也就是“皇室属性”或“君主身份”。它从“男性”这个概念中剥离了性别，留下了“皇室”的精髓。
   * `v_diff + vector('woman')`：现在，我们将这个代表“皇室属性”的关系向量，应用到“女性”这个概念上。
   * 结果：vector('woman') + "皇室属性" ≈ vector('queen')。这个结果非常直观：一个具有皇室属性的女性，就是“女王”。

  

* [OpenAI分词器](https://platform.openai.com/tokenizer)
* [BPE论文](https://arxiv.org/pdf/1508.07909)
* [word2vec论文](https://arxiv.org/pdf/1301.3781)
